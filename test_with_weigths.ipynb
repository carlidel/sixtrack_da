{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss comparison (DA vs. weighted particles)\n",
    "Given a regular radial scan performed with Sixtrack, we try different distributions and compare the lost amount of beam.\n",
    "\n",
    "## Set correct working directory and install libraries in SWAN instance\n",
    "(since SWAN generates a new instance of the notebook in another empty directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working in the right path\n",
    "%cd /eos/project/d/da-and-diffusion-studies/DA_Studies/Simulations/Models/da_sixtrack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the libraries\n",
    "import sys\n",
    "!{sys.executable} -m pip install --user tqdm pynverse sixtrackwrap\n",
    "!export PYTHONPATH=$CERNBOX_HOME/.local/lib/python3.7/site-packages:$PYTHONPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this \"presentation\" only! As some plotting parts execute a np.log10(0)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base libraries\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy.integrate as integrate\n",
    "from scipy.special import erf\n",
    "import pickle\n",
    "import itertools\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "from numba import njit, prange\n",
    "\n",
    "# Personal libraries\n",
    "#import sixtrackwrap_light as sx\n",
    "import sixtrackwrap as sx\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import matplotlib\n",
    "import matplotlib.ticker as ticker\n",
    "from math import gcd\n",
    "\n",
    "from scipy.special import lambertw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath = \"data/\"\n",
    "engine = sx.radial_scanner.load_values(savepath + \"big_scan.pkl\")\n",
    "\n",
    "min_turns = engine.min_time\n",
    "max_turns = engine.max_time\n",
    "n_turn_samples = 200\n",
    "\n",
    "turn_sampling = np.linspace(min_turns, max_turns, n_turn_samples, dtype=np.int_)[::-1]\n",
    "\n",
    "d_r = engine.dr\n",
    "starting_step = engine.starting_step\n",
    "\n",
    "# BASELINE COMPUTING\n",
    "baseline_samples = 33\n",
    "baseline_total_samples = baseline_samples ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_preliminary_values = np.linspace(-1.0, 1.0, baseline_samples)\n",
    "alpha_values = np.arccos(alpha_preliminary_values) / 2\n",
    "theta1_values = np.linspace(0.0, np.pi * 2.0, baseline_samples, endpoint=False)\n",
    "theta2_values = np.linspace(0.0, np.pi * 2.0, baseline_samples, endpoint=False)\n",
    "\n",
    "d_preliminar_alpha = alpha_preliminary_values[1] - alpha_preliminary_values[0]\n",
    "d_theta1 = theta1_values[1] - theta1_values[0]\n",
    "d_theta2 = theta2_values[1] - theta2_values[0]\n",
    "\n",
    "alpha_mesh, theta1_mesh, theta2_mesh = np.meshgrid(alpha_values, theta1_values, theta2_values, indexing='ij')\n",
    "\n",
    "alpha_flat = alpha_mesh.flatten()\n",
    "theta1_flat = theta1_mesh.flatten()\n",
    "theta2_flat = theta2_mesh.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Colormap for roughly visualize all the samples (regardless of the angle)\n",
    "Here we just observe all the gathered radial samples \"as a bunch\". (No sorting of any kind for the various starting angular conditions)\n",
    "\n",
    "I choose a logarithmic visualization of the stability value in order to better visualize the variation of the number of stable turns (white means absence of data, i.e. the particle was not simulated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efb5e8cf0c9441c2b5aa08c5aaee820f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carlidel/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log10\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "fig1, ax1 = plt.subplots()\n",
    "cmap = ax1.imshow(np.log10(engine.steps), aspect=\"auto\", extent=(engine.starting_step, engine.starting_step + engine.dr * len(engine.steps[0]),0,len(engine.steps)))\n",
    "ax1.set_ylabel(\"Radial sample number\")\n",
    "ax1.set_xlabel(\"Radial distance [normalized Sigma units]\")\n",
    "cbar = plt.colorbar(cmap)\n",
    "cbar.ax.set_ylabel(\"Number of stable turns $\\\\left[\\\\log_{10}(N_{turns})\\\\right]$\")\n",
    "ax1.set_title(\"Heatmap view of the radial scans\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring and visualizing 3D samples of DA!\n",
    "\n",
    "With this tool, you can (somewhat) visualize the angular dependencies of DA by moving the $\\theta_1$ and $\\theta_2$ sliders and setting up 3D samples of different dimension (the resulting sample is sample_size ** 3 big).\n",
    "\n",
    "What you will then visualize is the evolution of DA with the number of turns, considering different $\\alpha$ angles ($\\alpha$ indicates the central angle of the considered sample).\n",
    "\n",
    "**N.B.: the plotting process requires time, so after moving the sliders you will need to wait a little!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "146ac5bddcca424892a45d1a7749b711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19da7944c7b94222b51efd81fc745644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Number of turn samples to visualize'), IntSlider(value=2, continuous_update=False,…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f7a24fb26804fa98d6d8a0d889db23c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "cmap = matplotlib.cm.get_cmap('viridis')\n",
    "norm = matplotlib.colors.Normalize(vmin=np.log10(turn_sampling[-1]), vmax=np.log10(turn_sampling[0]))\n",
    "fig.colorbar(matplotlib.cm.ScalarMappable(norm=norm, cmap=cmap), label='Number of stable turns considered\\n$[\\\\log_{10}(N_{turns})]$')\n",
    "\n",
    "radiuses = engine.extract_DA(turn_sampling)\n",
    "radiuses = radiuses.reshape((baseline_samples, baseline_samples, baseline_samples, len(turn_sampling)))\n",
    "\n",
    "@njit\n",
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return idx\n",
    "\n",
    "@njit\n",
    "def take_sample(array, value, size):\n",
    "    assert size % 2 == 0\n",
    "    array = np.asarray(array)\n",
    "    idx = find_nearest(array, value)\n",
    "    if idx < size:\n",
    "        return 0, size\n",
    "    elif idx >= len(array) - size:\n",
    "        return len(array) - size, len(array)\n",
    "    else:\n",
    "        return idx - size // 2, idx + size // 2\n",
    "\n",
    "def update1(sample_size, th1, th2, n_to_visualize):\n",
    "    th1 *= np.pi\n",
    "    th2 *= np.pi\n",
    "    y_values = np.empty((len(range(sample_size, len(alpha_preliminary_values))), len(turn_sampling)))\n",
    "    x_values = np.empty((len(range(sample_size, len(alpha_preliminary_values)))))\n",
    "    x_err_values = np.empty((len(range(sample_size, len(alpha_preliminary_values)))))\n",
    "\n",
    "    th1_min, th1_max = take_sample(theta1_values, th1, sample_size)\n",
    "    th2_min, th2_max = take_sample(theta2_values, th2, sample_size)\n",
    "    theta1_sample = theta1_values[th1_min : th1_max]\n",
    "    theta2_sample = theta1_values[th2_min : th2_max]\n",
    "\n",
    "    mod_radiuses = np.power(radiuses, 4)[:, th1_min : th1_max, th2_min : th2_max]\n",
    "    mod_radiuses = integrate.simps(mod_radiuses, x=theta1_sample, axis=1)\n",
    "    mod_radiuses = integrate.simps(mod_radiuses, x=theta2_sample, axis=1)\n",
    "    \n",
    "    DA_whole = (\n",
    "        np.power(\n",
    "            mod_radiuses / (\n",
    "                (theta1_sample[-1] - theta1_sample[0]) \n",
    "                * (theta2_sample[-1] - theta2_sample[0])),\n",
    "            1/4\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for i, a_max in enumerate(range(sample_size, len(alpha_preliminary_values))):\n",
    "        a_min = a_max - sample_size\n",
    "        alpha_sample = alpha_preliminary_values[a_min : a_max]\n",
    "        a_mid = (alpha_values[a_min] + alpha_values[a_max]) / 2\n",
    "        \n",
    "        s_radiuses = mod_radiuses[a_min : a_max]\n",
    "        s_radiuses = integrate.simps(s_radiuses, x=alpha_sample, axis=0)\n",
    "\n",
    "        DA = (\n",
    "            np.power(\n",
    "                s_radiuses / (\n",
    "                    (alpha_sample[-1] - alpha_sample[0]) \n",
    "                    * (theta1_sample[-1] - theta1_sample[0]) \n",
    "                    * (theta2_sample[-1] - theta2_sample[0])),\n",
    "                1/4\n",
    "            )\n",
    "        )\n",
    "        y_values[i] = DA\n",
    "        x_values[i] = a_mid\n",
    "        x_err_values[i] = a_mid - alpha_values[a_min]\n",
    "        \n",
    "    y_values = np.asarray(y_values)\n",
    "    y_values = y_values.transpose()\n",
    "    x_values = np.asarray(x_values)\n",
    "    ax.clear()\n",
    "    for i in np.unique(np.logspace(0, np.log10(n_turn_samples), n_to_visualize, dtype=np.int)):\n",
    "    #for i in np.linspace(0, n_turn_samples, 5, dtype=np.int, endpoint=False):\n",
    "        if i == n_turn_samples:\n",
    "            i -= 1\n",
    "        value = np.log10(turn_sampling[i] - turn_sampling[-1]) / np.log10(turn_sampling[0] - turn_sampling[-1])\n",
    "        # whole stuff (integrated only over thetas)\n",
    "        ax.plot(alpha_values, DA_whole[:, i], c=cmap(value), linewidth=0.5, alpha=0.8)\n",
    "        # points\n",
    "        ax.errorbar(x_values, y_values[i], xerr=(x_err_values), linewidth=0, elinewidth=0.5, c=cmap(value), capsize=0.5, capthick=0.5, marker=\"*\", markeredgecolor=\"grey\")\n",
    "    ax.set_xlabel(\"$\\\\alpha$\")\n",
    "    ax.set_ylabel(\"Measured $DA$ in sample\")\n",
    "    ax.set_title(\"DA evolution over $\\\\alpha$ for a moving average of ${}^3$ elements (total is ${}^3$)\\nThis implies {} DA computations over the given $\\\\theta_1, \\\\theta_2$ slice.\\n$\\\\theta$ slice considered: $(\\\\theta_1 = {:.2f}\\\\pi, \\\\theta_2 = {:.2f}\\\\pi)$\".format(sample_size, baseline_samples, baseline_samples - sample_size, th1/np.pi, th2/np.pi, baseline_samples))\n",
    "    #ax.set_ylim(np.min(radiuses), np.max(radiuses))\n",
    "    ax.set_xlim(0.0, np.pi / 2.0)\n",
    "    ax.xaxis.set_major_formatter(\n",
    "        ticker.FuncFormatter(\n",
    "            lambda x, pos: (\"$\\\\frac{{{}}}{{{}}}$\".format(int(x/(np.pi/8)) // gcd(8, int(x/(np.pi/8))), 8 // gcd(8, int(x/(np.pi/8)))) if x != 0 else \"0\") + \"$\\\\pi$\"\n",
    "        )\n",
    "    )\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(base=np.pi/8))\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "a=widgets.IntSlider(value=4, min=2, max=baseline_samples - 4, step=2, continuous_update=False)\n",
    "b=widgets.FloatSlider(value=1, min=0, max=2 + 0.01, step=0.01, continuous_update=False)\n",
    "c=widgets.FloatSlider(value=1, min=0, max=2 + 0.01, step=0.01, continuous_update=False)\n",
    "d=widgets.IntSlider(value=2, min=2, max=n_turn_samples, step=1, continuous_update=False)\n",
    "ui = widgets.VBox([\n",
    "    widgets.Label(\"Number of turn samples to visualize\"), d,\n",
    "    widgets.Label(\"Size of the cubic sample\"), a,\n",
    "    widgets.Label(\"$\\\\theta_1$ value $[\\\\pi$ units$]$\"), b,\n",
    "    widgets.Label(\"$\\\\theta_2$ value $[\\\\pi$ units$]$\"), c])\n",
    "    \n",
    "out = widgets.interactive_output(\n",
    "    update1,\n",
    "    {\"sample_size\":a, \"th1\":b, \"th2\":c, \"n_to_visualize\":d}\n",
    ")\n",
    "\n",
    "display(ui, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A few notes on how to read the plot above\n",
    "* On the sampling of the $\\alpha$ angle: since in our 4D polar coordinates we have a non unitary jacobian for the $\\alpha$ variable, the sampling over $\\alpha$ was performed not uniformely, but it is balanced over the jacobian value (i.e. we sampled uniformely over the variable $y = \\cos 2\\alpha$, for $y \\in [-1,1]$), so that the amount of information obtained is maximized.\n",
    "* The continuous lines you see in the background of the plot are the average radial values computed for the corresponding $\\alpha$ angle and a sample of sample_sizeXsample_size dimension centered on the corresponding $\\theta_1, \\theta_2$ angles chosen.\n",
    "* The horizontal lines with dots represent an averaging integration over the corresponding $\\alpha$ interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same plot, but with a focus on $\\theta_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dedab61e55c41c59e641e87afaa1026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a08296b53b2b44b79b3a8bf402ea3e31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Number of turn samples to visualize'), IntSlider(value=2, continuous_update=False,…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fe46993c4184ab59fc1cb2c36e54267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig_th, ax_th = plt.subplots()\n",
    "cmap = matplotlib.cm.get_cmap('viridis')\n",
    "norm = matplotlib.colors.Normalize(vmin=np.log10(turn_sampling[-1]), vmax=np.log10(turn_sampling[0]))\n",
    "fig_th.colorbar(matplotlib.cm.ScalarMappable(norm=norm, cmap=cmap), label='Number of stable turns considered\\n$[\\\\log_{10}(N_{turns})]$')\n",
    "\n",
    "radiuses = engine.extract_DA(turn_sampling)\n",
    "radiuses = radiuses.reshape((baseline_samples, baseline_samples, baseline_samples, len(turn_sampling)))\n",
    "\n",
    "def update1(sample_size, alp, th2, n_to_visualize):\n",
    "    alp *= np.pi\n",
    "    th2 *= np.pi\n",
    "    y_values = np.empty((len(range(sample_size, len(alpha_preliminary_values))), len(turn_sampling)))\n",
    "    x_values = np.empty((len(range(sample_size, len(alpha_preliminary_values)))))\n",
    "    x_err_values = np.empty((len(range(sample_size, len(alpha_preliminary_values)))))\n",
    "\n",
    "    alp_min, alp_max = take_sample(alpha_values, alp, sample_size)\n",
    "    th2_min, th2_max = take_sample(theta2_values, th2, sample_size)\n",
    "    alpha_sample = alpha_values[alp_min : alp_max]\n",
    "    theta2_sample = theta1_values[th2_min : th2_max]\n",
    "\n",
    "    mod_radiuses = np.power(radiuses, 4)[alp_min : alp_max, : , th2_min : th2_max]\n",
    "    mod_radiuses = integrate.simps(mod_radiuses, x=alpha_sample, axis=0)\n",
    "    mod_radiuses = integrate.simps(mod_radiuses, x=theta2_sample, axis=1)\n",
    "    \n",
    "    DA_whole = (\n",
    "        np.power(\n",
    "            mod_radiuses / (\n",
    "                (alpha_sample[-1] - alpha_sample[0]) \n",
    "                * (theta2_sample[-1] - theta2_sample[0])),\n",
    "            1/4\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for i, th1_max in enumerate(range(sample_size, len(theta1_values))):\n",
    "        th1_min = th1_max - sample_size\n",
    "        theta1_sample = theta1_values[th1_min : th1_max]\n",
    "        th1_mid = (theta1_values[th1_min] + theta1_values[th1_max]) / 2\n",
    "        \n",
    "        s_radiuses = mod_radiuses[th1_min : th1_max]\n",
    "        s_radiuses = integrate.simps(s_radiuses, x=theta1_sample, axis=0)\n",
    "\n",
    "        DA = (\n",
    "            np.power(\n",
    "                s_radiuses / (\n",
    "                    (alpha_sample[-1] - alpha_sample[0]) \n",
    "                    * (theta1_sample[-1] - theta1_sample[0]) \n",
    "                    * (theta2_sample[-1] - theta2_sample[0])),\n",
    "                1/4\n",
    "            )\n",
    "        )\n",
    "        y_values[i] = DA\n",
    "        x_values[i] = th1_mid\n",
    "        x_err_values[i] = th1_mid - theta1_values[th1_min]\n",
    "        \n",
    "    y_values = np.asarray(y_values)\n",
    "    y_values = y_values.transpose()\n",
    "    x_values = np.asarray(x_values)\n",
    "    ax_th.clear()\n",
    "    for i in np.unique(np.logspace(0, np.log10(n_turn_samples), n_to_visualize, dtype=np.int)):\n",
    "    #for i in np.linspace(0, n_turn_samples, 5, dtype=np.int, endpoint=False):\n",
    "        if i == n_turn_samples:\n",
    "            i -= 1\n",
    "        value = np.log10(turn_sampling[i] - turn_sampling[-1]) / np.log10(turn_sampling[0] - turn_sampling[-1])\n",
    "        # whole stuff (integrated only over thetas)\n",
    "        ax_th.plot(theta1_values, DA_whole[:, i], c=cmap(value), linewidth=0.5, alpha=0.8)\n",
    "        # points\n",
    "        ax_th.errorbar(x_values, y_values[i], xerr=(x_err_values), linewidth=0, elinewidth=0.5, c=cmap(value), capsize=0.5, capthick=0.5, marker=\"*\", markeredgecolor=\"grey\")\n",
    "    ax_th.set_xlabel(\"$\\\\theta_1$\")\n",
    "    ax_th.set_ylabel(\"Measured $DA$ in sample\")\n",
    "    ax_th.set_title(\"DA evolution over $\\\\theta_1$ for a moving average of ${}^3$ elements (total is ${}^3$)\\nThis implies {} DA computations over the given $\\\\alpha$ slice.\\n$\\\\alpha , \\\\theta_2$ slice considered: $(\\\\alpha = {:.2f}\\\\pi, \\\\theta_2 = {:.2f}\\\\pi)$\".format(sample_size, baseline_samples, baseline_samples - sample_size, alp/np.pi, th2/np.pi, baseline_samples))\n",
    "    #ax_th.set_ylim(np.min(radiuses), np.max(radiuses))\n",
    "    ax_th.set_xlim(0.0, np.pi * 2.0)\n",
    "    ax_th.xaxis.set_major_formatter(\n",
    "        ticker.FuncFormatter(\n",
    "            lambda x, pos: (\"${{{}}}$\".format(int(x/np.pi)) + \"$\\\\pi$\")\n",
    "        )\n",
    "    )\n",
    "    ax_th.xaxis.set_major_locator(ticker.MultipleLocator(base=np.pi))\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "a=widgets.IntSlider(value=4, min=2, max=baseline_samples - 4, step=2, continuous_update=False)\n",
    "b=widgets.FloatSlider(value=0.25, min=0, max=0.5 + 0.01, step=0.01, continuous_update=False)\n",
    "c=widgets.FloatSlider(value=1, min=0, max=2 + 0.01, step=0.01, continuous_update=False)\n",
    "d=widgets.IntSlider(value=2, min=2, max=n_turn_samples, step=1, continuous_update=False)\n",
    "ui = widgets.VBox([\n",
    "    widgets.Label(\"Number of turn samples to visualize\"), d,\n",
    "    widgets.Label(\"Size of the cubic sample\"), a,\n",
    "    widgets.Label(\"$\\\\alpha$ value $[\\\\pi$ units$]$\"), b,\n",
    "    widgets.Label(\"$\\\\theta_2$ value $[\\\\pi$ units$]$\"), c])\n",
    "    \n",
    "out = widgets.interactive_output(\n",
    "    update1,\n",
    "    {\"sample_size\":a, \"alp\":b, \"th2\":c, \"n_to_visualize\":d}\n",
    ")\n",
    "\n",
    "display(ui, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same plot, but with a focus on $\\theta_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2afa640448f24acbb6f1948eed22906c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Number of turn samples to visualize'), IntSlider(value=2, continuous_update=False,…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e55af72a1204cee9c644c7b4519d145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig_th2, ax_th2 = plt.subplots()\n",
    "cmap = matplotlib.cm.get_cmap('viridis')\n",
    "norm = matplotlib.colors.Normalize(vmin=np.log10(turn_sampling[-1]), vmax=np.log10(turn_sampling[0]))\n",
    "fig_th2.colorbar(matplotlib.cm.ScalarMappable(norm=norm, cmap=cmap), label='Number of stable turns considered\\n$[\\\\log_{10}(N_{turns})]$')\n",
    "\n",
    "radiuses = engine.extract_DA(turn_sampling)\n",
    "radiuses = radiuses.reshape((baseline_samples, baseline_samples, baseline_samples, len(turn_sampling)))\n",
    "\n",
    "def update1(sample_size, alp, th2, n_to_visualize):\n",
    "    alp *= np.pi\n",
    "    th2 *= np.pi\n",
    "    y_values = np.empty((len(range(sample_size, len(alpha_preliminary_values))), len(turn_sampling)))\n",
    "    x_values = np.empty((len(range(sample_size, len(alpha_preliminary_values)))))\n",
    "    x_err_values = np.empty((len(range(sample_size, len(alpha_preliminary_values)))))\n",
    "\n",
    "    alp_min, alp_max = take_sample(alpha_values, alp, sample_size)\n",
    "    th2_min, th2_max = take_sample(theta2_values, th2, sample_size)\n",
    "    alpha_sample = alpha_values[alp_min : alp_max]\n",
    "    theta2_sample = theta1_values[th2_min : th2_max]\n",
    "\n",
    "    mod_radiuses = np.power(radiuses, 4)[alp_min : alp_max, th2_min : th2_max, :]\n",
    "    mod_radiuses = integrate.simps(mod_radiuses, x=alpha_sample, axis=0)\n",
    "    mod_radiuses = integrate.simps(mod_radiuses, x=theta2_sample, axis=0)\n",
    "    \n",
    "    DA_whole = (\n",
    "        np.power(\n",
    "            mod_radiuses / (\n",
    "                (alpha_sample[-1] - alpha_sample[0]) \n",
    "                * (theta2_sample[-1] - theta2_sample[0])),\n",
    "            1/4\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for i, th1_max in enumerate(range(sample_size, len(theta1_values))):\n",
    "        th1_min = th1_max - sample_size\n",
    "        theta1_sample = theta1_values[th1_min : th1_max]\n",
    "        th1_mid = (theta1_values[th1_min] + theta1_values[th1_max]) / 2\n",
    "        \n",
    "        s_radiuses = mod_radiuses[th1_min : th1_max]\n",
    "        s_radiuses = integrate.simps(s_radiuses, x=theta1_sample, axis=0)\n",
    "\n",
    "        DA = (\n",
    "            np.power(\n",
    "                s_radiuses / (\n",
    "                    (alpha_sample[-1] - alpha_sample[0]) \n",
    "                    * (theta1_sample[-1] - theta1_sample[0]) \n",
    "                    * (theta2_sample[-1] - theta2_sample[0])),\n",
    "                1/4\n",
    "            )\n",
    "        )\n",
    "        y_values[i] = DA\n",
    "        x_values[i] = th1_mid\n",
    "        x_err_values[i] = th1_mid - theta1_values[th1_min]\n",
    "        \n",
    "    y_values = np.asarray(y_values)\n",
    "    y_values = y_values.transpose()\n",
    "    x_values = np.asarray(x_values)\n",
    "    ax_th2.clear()\n",
    "    for i in np.unique(np.logspace(0, np.log10(n_turn_samples), n_to_visualize, dtype=np.int)):\n",
    "    #for i in np.linspace(0, n_turn_samples, 5, dtype=np.int, endpoint=False):\n",
    "        if i == n_turn_samples:\n",
    "            i -= 1\n",
    "        value = np.log10(turn_sampling[i] - turn_sampling[-1]) / np.log10(turn_sampling[0] - turn_sampling[-1])\n",
    "        # whole stuff (integrated only over thetas)\n",
    "        ax_th2.plot(theta1_values, DA_whole[:, i], c=cmap(value), linewidth=0.5, alpha=0.8)\n",
    "        # points\n",
    "        ax_th2.errorbar(x_values, y_values[i], xerr=(x_err_values), linewidth=0, elinewidth=0.5, c=cmap(value), capsize=0.5, capthick=0.5, marker=\"*\", markeredgecolor=\"grey\")\n",
    "    ax_th2.set_xlabel(\"$\\\\theta_2$\")\n",
    "    ax_th2.set_ylabel(\"Measured $DA$ in sample\")\n",
    "    ax_th2.set_title(\"DA evolution over $\\\\theta_2$ for a moving average of ${}^3$ elements (total is ${}^3$)\\nThis implies {} DA computations over the given $\\\\theta_2$ slice.\\n$\\\\alpha , \\\\theta_1$ slice considered: $(\\\\alpha = {:.2f}\\\\pi, \\\\theta_1 = {:.2f}\\\\pi)$\".format(sample_size, baseline_samples, baseline_samples - sample_size, alp/np.pi, th2/np.pi, baseline_samples))\n",
    "    #ax_th2.set_ylim(np.min(radiuses), np.max(radiuses))\n",
    "    ax_th2.set_xlim(0.0, np.pi * 2.0)\n",
    "    ax_th2.xaxis.set_major_formatter(\n",
    "        ticker.FuncFormatter(\n",
    "            lambda x, pos: (\"${{{}}}$\".format(int(x/np.pi)) + \"$\\\\pi$\")\n",
    "        )\n",
    "    )\n",
    "    ax_th2.xaxis.set_major_locator(ticker.MultipleLocator(base=np.pi))\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "a=widgets.IntSlider(value=4, min=2, max=baseline_samples - 4, step=2, continuous_update=False)\n",
    "b=widgets.FloatSlider(value=0.25, min=0, max=0.5 + 0.01, step=0.01, continuous_update=False)\n",
    "c=widgets.FloatSlider(value=1, min=0, max=2 + 0.01, step=0.01, continuous_update=False)\n",
    "d=widgets.IntSlider(value=2, min=2, max=n_turn_samples, step=1, continuous_update=False)\n",
    "ui = widgets.VBox([\n",
    "    widgets.Label(\"Number of turn samples to visualize\"), d,\n",
    "    widgets.Label(\"Size of the cubic sample\"), a,\n",
    "    widgets.Label(\"$\\\\alpha$ value $[\\\\pi$ units$]$\"), b,\n",
    "    widgets.Label(\"$\\\\theta_1$ value $[\\\\pi$ units$]$\"), c])\n",
    "    \n",
    "out = widgets.interactive_output(\n",
    "    update1,\n",
    "    {\"sample_size\":a, \"alp\":b, \"th2\":c, \"n_to_visualize\":d}\n",
    ")\n",
    "\n",
    "display(ui, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup for loss comparison analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "radiuses = engine.extract_DA(turn_sampling)\n",
    "radiuses = radiuses.reshape((baseline_samples, baseline_samples, baseline_samples, len(turn_sampling)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "DA = []\n",
    "\n",
    "mod_radiuses = radiuses.copy()\n",
    "mod_radiuses = np.power(radiuses, 4)\n",
    "mod_radiuses1 = integrate.simps(mod_radiuses, x=theta1_values, axis=1)\n",
    "mod_radiuses2 = integrate.simps(mod_radiuses1, x=theta2_values, axis=1)\n",
    "mod_radiuses3 = integrate.simps(mod_radiuses2, x=alpha_preliminary_values, axis=0)\n",
    "\n",
    "for i in range(len(turn_sampling)):\n",
    "    DA.append(\n",
    "        np.power(\n",
    "            mod_radiuses3[i] / (2 * theta1_values[-1] * theta2_values[-1]),\n",
    "            1/4\n",
    "        )\n",
    "    )\n",
    "\n",
    "DA = np.asarray(DA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "axis_sampling = np.concatenate((turn_sampling,[0.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine_analyzer = sx.uniform_radial_scanner(baseline_samples, engine.steps, engine.dr, engine.starting_step)\n",
    "uniform_engine = sx.uniform_scanner.load_values(\"data/big_uniform_scan_small.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A little sanity test for the loosing measurement system\n",
    "\n",
    "Right now, we have two kinds of scans:\n",
    "1. Radial scan\n",
    "2. Uniform scan\n",
    "\n",
    "And we want to be sure that the two methods returns the same lost value for equal boolean masks.\n",
    "Here, we compare the lost values obtained by considering 4D spheres of active particles of different radiuses. As beam distribution, we consider a uniform beam.\n",
    "\n",
    "The values are compared with the expected theoretical result (i.e. the corresponding 4D sphere hypervolume).\n",
    "\n",
    "#### Assign a uniform weight distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine_analyzer.assign_weights()\n",
    "uniform_engine.assign_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the direct comparison\n",
    "As we can see here, the sanity test is valid.\n",
    "\n",
    "However, we can observe the increasing effects of the stronger discretization on radial scan. This suggest that, while setting up the baseline value for our lost beam analysis, we must choose a \"good\" value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b523698e180249c7995e0e9bfbc06c8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=501.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "a = []\n",
    "b = []\n",
    "theoretical = []\n",
    "x = np.linspace(15, 35, 501)\n",
    "for i in tqdm(x):\n",
    "    a.append(engine_analyzer.compute_loss_cut(i))\n",
    "    b.append(uniform_engine.compute_loss_cut(i))\n",
    "    theoretical.append(i ** 4 * np.pi * np.pi / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb43ba176c2346b0962fa046aef32dd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Sanity test 1')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig_test, ax_test = plt.subplots()\n",
    "ax_test.plot(x, np.abs(a), label=\"Radial scan\")\n",
    "ax_test.plot(x, np.abs(b), label=\"Uniform scan\")\n",
    "ax_test.plot(x, theoretical, label=\"Theoretical value\", linewidth=0.8)\n",
    "ax_test.legend()\n",
    "ax_test.set_xlabel(\"Radius of the active hypersphere [Normalized units]\")\n",
    "ax_test.set_ylabel(\"Hypervolume measured [A.U.]\")\n",
    "ax_test.set_title(\"Sanity test 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the relative error between the radial scan results and the expected theoretical results: the discretization effects are well classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1443c921ac524442a399c3a447e833ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'evolution of the relative error (radial scan vs. theoretical value)')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig_test_2, ax_test_2 = plt.subplots()\n",
    "ax_test_2.plot(x, (np.asarray(a) - np.asarray(theoretical))/np.asarray(theoretical))\n",
    "ax_test_2.set_xlabel(\"Radius of the active hypersphere [Normalized units]\")\n",
    "ax_test_2.set_ylabel(\"Relative error\")\n",
    "ax_test_2.set_title(\"evolution of the relative error (radial scan vs. theoretical value)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_point = 30.48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How is the error on the DA loss computed right now?\n",
    "\n",
    "1. Consider all the radiuses sampled.\n",
    "2. Compute the DA value.\n",
    "3. For every radius sampled, compute the difference from the DA value.\n",
    "4. The absolute value of the average of all these differences is considered as error.\n",
    "\n",
    "(I tried using the Standard Deviation of the radiuses distribution, but it ended up being 10% of the DA itself, so we \"need\" somehow a smaller error estimation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uniform distribution case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_from_DA(da_list, da_cut):\n",
    "    temp = np.pi ** 2 / 2 * np.power(da_list, 4)\n",
    "    return np.concatenate((temp, [np.pi ** 2 / 2 * np.power(da_cut, 4)]))\n",
    "\n",
    "values = loss_from_DA(DA, cut_point)\n",
    "values /= values[-1]\n",
    "\n",
    "# Error computing\n",
    "\n",
    "values1 = loss_from_DA(DA - np.absolute(np.mean(radiuses - DA, axis=(0,1,2))), cut_point)\n",
    "values1 /= values1[-1]\n",
    "\n",
    "values2 = loss_from_DA(DA + np.absolute(np.mean(radiuses - DA, axis=(0,1,2))), cut_point)\n",
    "values2 /= values2[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine_analyzer.assign_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniform_engine.assign_weights(radial_cut=cut_point)\n",
    "real_values = engine_analyzer.compute_loss(turn_sampling, cut_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9daa224248ba4464aebdf9a42dfee1e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Uniform beam (Cutting Point at $DA=30.48$)')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig3, ax3 = plt.subplots()\n",
    "ax3.plot(axis_sampling, values, label=\"Values from DA\")\n",
    "ax3.fill_between(axis_sampling, values1, values2, label=\"Values from DA - error\", color=\"C0\", alpha=0.4)\n",
    "ax3.plot(turn_sampling, real_values, label=\"Values from weights\")\n",
    "ax3.plot(turn_sampling, uniform_engine.compute_loss(turn_sampling), label=\"Values from uniform sampling\")\n",
    "ax3.legend()\n",
    "ax3.set_xlabel(\"$N$ turns\")\n",
    "ax3.set_ylabel(\"Active beam\")\n",
    "ax3.set_title(\"Uniform beam (Cutting Point at $DA={}$)\".format(cut_point))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian with slider available for $\\sigma$\n",
    "\n",
    "With the slider it is possible to regulate the $\\sigma$ value of the 4D gaussian beam distribution.\n",
    "\n",
    "It is possible to observe how extreme values for $\\sigma$ strongly changes the two loss behaviours.\n",
    "\n",
    "**N.B.: Place the slider at the desired position and then press the button!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7689acf5191e438d80ce2f8a26a684cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06535e16a8a040c4b437e073c5758fe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=15.0, description='sigma', max=30.0, min=1.0, step=0.5), Button(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.update2(sigma)>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig2, ax2 = plt.subplots()\n",
    "\n",
    "def update2(sigma):\n",
    "    ax2.clear()\n",
    "    # Cursed Sanity Check\n",
    "    def loss_from_DA(DA_list, DA_max):\n",
    "        temp = - np.exp(- ((DA_list / sigma) ** 2) / 2) * (DA_list ** 2 + 2 * sigma ** 2) + 2 * sigma ** 2\n",
    "        return np.concatenate((temp, [- np.exp(- ((DA_max / sigma) ** 2) / 2) * (DA_max ** 2 + 2 * sigma ** 2) + 2 * sigma ** 2]))\n",
    "\n",
    "    values = loss_from_DA(DA, cut_point)\n",
    "    values /= values[-1]\n",
    "    values[-20:]\n",
    "\n",
    "    # Error computing\n",
    "    \n",
    "    values1 = loss_from_DA(DA - np.absolute(np.mean(radiuses - DA, axis=(0,1,2))), cut_point)\n",
    "    values1 /= values1[-1]\n",
    "\n",
    "    values2 = loss_from_DA(DA + np.absolute(np.mean(radiuses - DA, axis=(0,1,2))), cut_point)\n",
    "    values2 /= values2[-1]\n",
    "\n",
    "    engine_analyzer.assign_weights(\n",
    "        sx.assign_symmetric_gaussian(sigma)\n",
    "    )\n",
    "    uniform_engine.assign_weights(\n",
    "        f=lambda x, px, y, py : (\n",
    "            np.exp(-0.5 * (np.power(x / sigma, 2.0) + np.power(y / sigma, 2.0) + np.power(py / sigma, 2.0) + np.power(px / sigma, 2.0)))\n",
    "        ),\n",
    "        radial_cut=cut_point\n",
    "    )\n",
    "\n",
    "    real_values = engine_analyzer.compute_loss(turn_sampling, cut_point)\n",
    "    unif_values = uniform_engine.compute_loss(turn_sampling)\n",
    "    ax2.plot(axis_sampling, values, label=\"Values from DA\")\n",
    "    ax2.fill_between(axis_sampling, values1, values2, color=\"C0\", alpha=0.4)\n",
    "    ax2.plot(turn_sampling, real_values, label=\"Values from radial scan\")\n",
    "    ax2.plot(turn_sampling, unif_values, label=\"Values from uniform scan\")\n",
    "    ax2.legend()\n",
    "    ax2.set_xlabel(\"$N$ turns\")\n",
    "    ax2.set_ylabel(\"Active beam\")\n",
    "    ax2.set_title(\"Symmetric gaussian beam\\n($\\\\sigma = {}$, cutting Point at $DA={}$)\".format(sigma, cut_point))\n",
    "    plt.tight_layout()\n",
    "\n",
    "widgets.interact_manual(update2, sigma=widgets.FloatSlider(value=15, min=1, max=30, step=0.5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A contour plot for analyzing the N turns / Sigma relation in the loss difference between real values and DA-based values for a symmetric gaussian distribution\n",
    "\n",
    "## Data computation (this takes a long time!)\n",
    "Here we compute the loss for many different values of sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15c84e01b96242178d5ac26c1f28d322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sigma_samples = 100\n",
    "sigma_list = np.linspace(2.0, 30, sigma_samples)\n",
    "\n",
    "#unif_values = np.empty((sigma_samples, len(turn_sampling)))\n",
    "r_values = np.empty((sigma_samples, len(turn_sampling)))\n",
    "DA_values = np.empty((sigma_samples, len(axis_sampling)))\n",
    "\n",
    "@njit\n",
    "def loss_from_DA(DA_list, DA_max, sigma):\n",
    "        temp = - np.exp(- ((DA_list / sigma) ** 2) / 2) * (DA_list ** 2 + 2 * sigma ** 2) + 2 * sigma ** 2\n",
    "        temp = np.concatenate((temp, np.array([- np.exp(- ((DA_max / sigma) ** 2) / 2) * (DA_max ** 2 + 2 * sigma ** 2) + 2 * sigma ** 2])))\n",
    "        return temp\n",
    "    \n",
    "    \n",
    "for i, sigma in tqdm(enumerate(sigma_list), total=len(sigma_list)):\n",
    "    values = loss_from_DA(DA, cut_point, sigma)\n",
    "    values /= values[-1]\n",
    "    DA_values[i] = values\n",
    "    engine_analyzer.assign_weights(\n",
    "        sx.assign_symmetric_gaussian(sigma)\n",
    "    )\n",
    "    #uniform_engine.assign_weights(\n",
    "    #    f=lambda x, px, y, py : (\n",
    "    #        np.exp(-0.5 * (np.power(x / sigma, 2.0) + np.power(y / sigma, 2.0) + np.power(py / sigma, 2.0) + np.power(px / sigma, 2.0)))\n",
    "    #    ),\n",
    "    #    radial_cut=27\n",
    "    #)\n",
    "\n",
    "    real_values = engine_analyzer.compute_loss(turn_sampling, cut_point)\n",
    "    r_values[i] = real_values\n",
    "    #u_values = uniform_engine.compute_loss(turn_sampling)\n",
    "    #unif_values[i] = u_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualization\n",
    "### Two colormaps for basic comparison\n",
    "Here we visualize how the relative loss changes depending on the $\\sigma$ value of the 4D gaussian distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7f64d66de050>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig4, axs4 = plt.subplots(ncols=2)\n",
    "\n",
    "global_min = min(np.min(r_values), np.min(DA_values))\n",
    "\n",
    "axs4[0].imshow(r_values[:,::-1], aspect='auto', extent=(turn_sampling[-1], turn_sampling[0], sigma_list[0], sigma_list[-1]), vmin=global_min, vmax=1)\n",
    "axs4[0].set_xlabel(\"$N$ turns\")\n",
    "axs4[0].set_ylabel(\"$\\\\sigma$ value\")\n",
    "axs4[0].set_title(\"Loss from actual lost particles\")\n",
    "\n",
    "im = axs4[1].imshow(DA_values[:,::-1], aspect='auto', extent=(axis_sampling[-1], axis_sampling[0], sigma_list[0], sigma_list[-1]), vmin=global_min, vmax=1)\n",
    "axs4[1].set_xlabel(\"$N$ turns\")\n",
    "axs4[1].set_ylabel(\"$\\\\sigma$ value\")\n",
    "axs4[1].set_title(\"Loss from DA extrapolation\")\n",
    "\n",
    "fig4.subplots_adjust(right=0.85)\n",
    "cbar_ax = fig4.add_axes([0.90, 0.15, 0.02, 0.7])\n",
    "fig4.colorbar(im, cax=cbar_ax, label=\"Relative loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A contour plot for the difference evolution\n",
    "\n",
    "Here we visualize the evolution of the relative error value:\n",
    "\n",
    "$$ \\frac{|\\text{Loss}_{\\text{true}}-\\text{Loss}_{\\text{DA}}|}{\\text{Loss}_{\\text{true}}}$$\n",
    "\n",
    "**N.B.** for extremely low $\\sigma$ values we degenerate to a 0/0 error since we register almost no loss at all with the scanning we are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac2a981431c048aea64bd0ad23b44e27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Relative difference between real loss and DA loss')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig5, ax5 = plt.subplots()\n",
    "skip = -50\n",
    "XX, YY = np.meshgrid(turn_sampling, sigma_list)\n",
    "img = ax5.contourf(XX[:,:skip], YY[:,:skip], (np.absolute(DA_values[:,:-1] - r_values)/r_values)[:,:skip], levels=20)\n",
    "fig5.colorbar(img, label=\"Relative error\")\n",
    "ax5.grid()\n",
    "ax5.set_xlabel(\"$N$ turns\")\n",
    "ax5.set_ylabel(\"$\\\\sigma$ value\")\n",
    "ax5.set_title(\"Relative difference between real loss and DA loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61ca5090609147c08d397b302eb6c2c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Relative difference between real loss and DA loss\\n(More focus on the lower part!)')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig5, ax5 = plt.subplots()\n",
    "skip = -50\n",
    "XX, YY = np.meshgrid(turn_sampling, sigma_list)\n",
    "img = ax5.contourf(XX[:,:skip], YY[:,:skip], (np.absolute(DA_values[:,:-1] - r_values)/r_values)[:,:skip], levels=100, vmax=0.032)\n",
    "fig5.colorbar(img, label=\"Relative error\")\n",
    "ax5.grid()\n",
    "ax5.set_xlabel(\"$N$ turns\")\n",
    "ax5.set_ylabel(\"$\\\\sigma$ value\")\n",
    "ax5.set_title(\"Relative difference between real loss and DA loss\\n(More focus on the lower part!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Remarks:\n",
    "1. We have perfect coherence between the radial and the uniform losses\n",
    "2. We know that the baseline value choice is critical\n",
    "3. The various discrepancies are confirmed (and analyzed in the other notebook)\n",
    "4. The $\\sigma$ behaviour is confirmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
