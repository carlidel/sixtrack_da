{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Radial Average\n",
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base libraries\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy.integrate as integrate\n",
    "from tqdm import tqdm\n",
    "from scipy.special import erf\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "from SALib.sample import saltelli\n",
    "from SALib.analyze import sobol\n",
    "\n",
    "# Personal libraries\n",
    "import sixtrackwrap as sx\n",
    "\n",
    "from parameters import *\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active radiuses: 2049 / 2049\n",
      "Sample size per active radius: 3\n",
      "Expected execution time for step: nan\n",
      "Elapsed time for whole iteration: 32.09\n",
      "Time per single iteration: 5.221152084521866e-05\n",
      "r: 12.0 . Turns to do: 100 . Min found: 100\n",
      "Active radiuses: 2049 / 2049\n",
      "Sample size per active radius: 3\n",
      "Expected execution time for step: 32.09\n",
      "Elapsed time for whole iteration: 32.08\n",
      "Time per single iteration: 5.218078978802104e-05\n",
      "r: 15.0 . Turns to do: 100 . Min found: 100\n",
      "Active radiuses: 2049 / 2049\n",
      "Sample size per active radius: 3\n",
      "Expected execution time for step: 32.08\n",
      "Elapsed time for whole iteration: 32.14\n",
      "Time per single iteration: 5.228652708972976e-05\n",
      "r: 18.0 . Turns to do: 100 . Min found: 100\n",
      "Active radiuses: 2049 / 2049\n",
      "Sample size per active radius: 3\n",
      "Expected execution time for step: 32.14\n",
      "Elapsed time for whole iteration: 31.99\n",
      "Time per single iteration: 5.2034343687212326e-05\n",
      "r: 21.0 . Turns to do: 100 . Min found: 50\n",
      "Active radiuses: 2049 / 2049\n",
      "Sample size per active radius: 3\n",
      "Expected execution time for step: 31.99\n",
      "Elapsed time for whole iteration: 28.24\n",
      "Time per single iteration: 4.59442061875261e-05\n",
      "r: 24.0 . Turns to do: 100 . Min found: 6\n",
      "Active radiuses: 1937 / 2049\n",
      "Sample size per active radius: 3\n",
      "Expected execution time for step: 26.70\n",
      "Elapsed time for whole iteration: 11.65\n",
      "Time per single iteration: 2.0051983501964092e-05\n",
      "r: 27.0 . Turns to do: 100 . Min found: 4\n",
      "Active radiuses: 1807 / 2049\n",
      "Sample size per active radius: 3\n",
      "Expected execution time for step: 10.87\n",
      "Elapsed time for whole iteration: 10.68\n",
      "Time per single iteration: 1.9694851060961252e-05\n",
      "r: 30.0 . Turns to do: 100 . Min found: 6\n",
      "Active radiuses: 1367 / 2049\n",
      "Sample size per active radius: 4\n",
      "Expected execution time for step: 10.77\n",
      "Elapsed time for whole iteration: 24.45\n",
      "Time per single iteration: 4.472277377239526e-05\n",
      "r: 34.0 . Turns to do: 28 . Min found: 1\n",
      "Active radiuses: 494 / 2049\n",
      "Sample size per active radius: 11\n",
      "Expected execution time for step: 6.80\n",
      "Elapsed time for whole iteration: 2.55\n",
      "Time per single iteration: 1.6734493700786937e-05\n",
      "r: 45.0 . Turns to do: 3 . Min found: 0\n",
      "TOTAL ELAPSED TIME IN SECONDS: 205.87\n"
     ]
    }
   ],
   "source": [
    "error_2 = {}\n",
    "DA_2 = {}\n",
    "raw_error_2 = {}\n",
    "DA_5 = {}\n",
    "error_5 = {}\n",
    "\n",
    "alpha_preliminary_values = np.linspace(-1.0, 1.0, samples)\n",
    "alpha_values = np.arccos(alpha_preliminary_values) / 2\n",
    "d_preliminar_alpha = alpha_preliminary_values[1] - alpha_preliminary_values[0]\n",
    "\n",
    "# Extracting the radiuses with theta1 = theta2 = 0.0\n",
    "\n",
    "time_start = time.time()\n",
    "\n",
    "engine = sx.radial_scanner(\n",
    "    alpha_values, \n",
    "    np.zeros(alpha_values.shape),\n",
    "    np.zeros(alpha_values.shape),\n",
    "    d_r,\n",
    "    starting_step=starting_step\n",
    ")\n",
    "engine.scan(turn_sampling[0], turn_sampling[-1], batch_size=batch_size)\n",
    "all_radiuses = engine.extract_DA(turn_sampling)\n",
    "\n",
    "elapsed_time_engine = time.time() - time_start\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D simple integral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "skips = [1]\n",
    "while True:\n",
    "    if (samples - 1) // skips[-1] > 4:\n",
    "        skips.append(skips[-1] * 2)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "for i in skips:\n",
    "    alpha = alpha_preliminary_values[::i]\n",
    "    cutted_radiuses = all_radiuses[::i]\n",
    "    value = integrate.simps(cutted_radiuses ** 2, alpha, axis=0)\n",
    "    less_value = integrate.simps(cutted_radiuses[::2] ** 2, alpha[::2], axis=0)\n",
    "    uncertainty = np.abs((value - less_value))\n",
    "\n",
    "    DA = np.sqrt(value / 2)\n",
    "    uncertainty = 0.5 * np.power(value / 2, -0.5) * uncertainty\n",
    "    DA_5[cutted_radiuses.shape] = np.asarray(DA)\n",
    "    error_5[cutted_radiuses.shape] = uncertainty "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual angular averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A/home/carlidel/Insync/carlo.montanari3@studio.unibo.it/OneDrive Biz/optimized_code/sixtrackwrap/sixtrackwrap/__init__.py:68: RuntimeWarning: invalid value encountered in true_divide\n",
      "  matrices = np.nansum(matrices.reshape(\n",
      "\n",
      " 20%|██        | 1/5 [00:34<02:17, 34.37s/it]\u001b[A\n",
      " 40%|████      | 2/5 [01:00<01:35, 31.93s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [01:19<00:56, 28.16s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [01:31<00:23, 23.20s/it]\u001b[A\n",
      "100%|██████████| 5/5 [01:36<00:00, 19.25s/it]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "values = []\n",
    "indexes = []\n",
    "raw_values = []\n",
    "refined_values = []\n",
    "    \n",
    "for i in tqdm(range(len(turn_sampling))):\n",
    "    radiuses = all_radiuses[:, i]\n",
    "\n",
    "    r, a, th1, th2 = sx.full_track_particles(\n",
    "        radiuses,\n",
    "        alpha_values,\n",
    "        np.zeros(alpha_values.shape),\n",
    "        np.zeros(alpha_values.shape),\n",
    "        turn_sampling[i])\n",
    "\n",
    "    count_matrix, avg_matrix, _ = sx.accumulate_and_return(r, a, th1, th2, n_subdivisions)\n",
    "    \n",
    "    if i == 0:\n",
    "        with open(savepath + \"data/matrices_2.pkl\", 'wb') as f:\n",
    "            pickle.dump((count_matrix, avg_matrix), f, protocol=4)\n",
    "    \n",
    "    _, _, result_total, validity_total = sx.recursive_accumulation(count_matrix, avg_matrix)\n",
    "    \n",
    "    values.append(result_total)\n",
    "    indexes.append(np.argmax(validity_total, axis=0))\n",
    "    refined_values.append([result_total[indexes[-1][i]][i] for i in range(len(indexes[-1]))])\n",
    "    raw_values.append(np.average(np.power(r, 4), axis=1))\n",
    "        \n",
    "steps = [1]\n",
    "while True:\n",
    "    if (values[0][0].shape[0] - 1) / steps[-1] > 4:\n",
    "        steps.append(steps[-1] * 2)\n",
    "    else:\n",
    "        break\n",
    "    \n",
    "for jump in steps:\n",
    "    DA = []\n",
    "    error = []\n",
    "    DA_mc = []\n",
    "    raw_error_mc = []\n",
    "    error_mc = []\n",
    "    for i in range(len(turn_sampling)):\n",
    "        DA.append(np.power(integrate.simps(refined_values[i][::jump], alpha_preliminary_values[::jump]) * 0.5, 1/4))\n",
    "        temp = np.power(integrate.simps(refined_values[i][::jump * 2], alpha_preliminary_values[::jump * 2]) * 0.5, 1/4)\n",
    "        error.append(np.absolute(DA[-1] - temp))\n",
    "\n",
    "        DA_mc.append(np.power(np.average(refined_values[i][::jump]), 1/4))\n",
    "        raw_error_mc.append(np.std(refined_values[i][::jump]))\n",
    "        error_mc.append(0.25 * np.power(DA_mc[-1], -3) * np.std(values[i][::jump]) / np.sqrt(np.size(values[i][::jump])))\n",
    "\n",
    "    DA_2[( len(refined_values[i][::jump]), \"refined\", \"int\")] = DA\n",
    "    error_2[( len(refined_values[i][::jump]), \"refined\", \"int\")] = error\n",
    "    DA_2[( len(refined_values[i][::jump]), \"refined\", \"mc\")] = DA_mc\n",
    "    error_2[( len(refined_values[i][::jump]), \"refined\", \"mc\")] = error_mc\n",
    "    raw_error_2[( len(refined_values[i][::jump]), \"refined\", \"mc\")] = raw_error_mc\n",
    "\n",
    "    DA = []\n",
    "    error = []\n",
    "    DA_mc = []\n",
    "    raw_error_mc = []\n",
    "    error_mc = []\n",
    "    for i in range(len(turn_sampling)):\n",
    "        DA.append(np.power(integrate.simps(raw_values[i][::jump], alpha_preliminary_values[::jump]) * 0.5, 1/4))\n",
    "        temp = np.power(integrate.simps(raw_values[i][::jump * 2], alpha_preliminary_values[::jump * 2]) * 0.5, 1/4)\n",
    "        error.append(np.absolute(DA[-1] - temp))\n",
    "\n",
    "        DA_mc.append(np.power(np.average(raw_values[i][::jump]), 1/4))\n",
    "        raw_error_mc.append(np.std(raw_values[i][::jump]))\n",
    "        error_mc.append(0.25 * np.power(DA_mc[-1], -3) * np.std(values[i][::jump]) / np.sqrt(np.size(values[i][::jump])))\n",
    "\n",
    "    DA_2[( len(raw_values[i][::jump]), \"raw\", \"int\")] = DA\n",
    "    error_2[( len(raw_values[i][::jump]), \"raw\", \"int\")] = error\n",
    "    DA_2[( len(raw_values[i][::jump]), \"raw\", \"mc\")] = DA_mc\n",
    "    error_2[( len(raw_values[i][::jump]), \"raw\", \"mc\")] = error_mc\n",
    "    raw_error_2[( len(raw_values[i][::jump]), \"raw\", \"mc\")] = raw_error_mc\n",
    "\n",
    "    for j in range(len(values[0])):\n",
    "        DA = []\n",
    "        error = []\n",
    "        DA_mc = []\n",
    "        raw_error_mc = []\n",
    "        error_mc = []\n",
    "        for i in range(len(turn_sampling)):\n",
    "            DA.append(np.power(integrate.simps(values[i][j][::jump], alpha_preliminary_values[::jump]) * 0.5, 1/4))\n",
    "            temp = np.power(integrate.simps(values[i][j][::jump * 2], alpha_preliminary_values[::jump * 2]) * 0.5, 1/4)\n",
    "            error.append(np.absolute(DA[-1] - temp))\n",
    "\n",
    "            DA_mc.append(np.power(np.average(values[i][j][::jump]), 1/4))\n",
    "            raw_error_mc.append(np.std(values[i][j][::jump]))\n",
    "            error_mc.append(0.25 * np.power(DA_mc[-1], -3) * np.std(values[i][j][::jump]) / np.sqrt(np.size(values[i][j][::jump])))\n",
    "\n",
    "        DA_2[( len(values[i][j][::jump]), 2 ** (j), \"int\")] = DA\n",
    "        error_2[( len(values[i][j][::jump]), 2 ** (j), \"int\")] = error\n",
    "        DA_2[( len(values[i][j][::jump]), 2 ** (j), \"mc\")] = DA_mc\n",
    "        error_2[( len(values[i][j][::jump]), 2 ** (j), \"mc\")] = error_mc\n",
    "        raw_error_2[( len(values[i][j][::jump]), 2 ** (j), \"mc\")] = raw_error_mc\n",
    "        \n",
    "elapsed_time_processing = time.time() - time_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(savepath + \"data/DA_2.pkl\", 'wb') as f:\n",
    "    pickle.dump(DA_2, f, protocol=4)\n",
    "    \n",
    "with open(savepath + \"data/error_2.pkl\", 'wb') as f:\n",
    "    pickle.dump(error_2, f, protocol=4)\n",
    "    \n",
    "with open(savepath + \"data/raw_error_2.pkl\", 'wb') as f:\n",
    "    pickle.dump(raw_error_2, f, protocol=4)\n",
    "    \n",
    "with open(savepath + \"data/DA_5.pkl\", 'wb') as f:\n",
    "    pickle.dump(DA_5, f, protocol=4)\n",
    "    \n",
    "with open(savepath + \"data/error_5.pkl\", 'wb') as f:\n",
    "    pickle.dump(error_5, f, protocol=4)\n",
    "\n",
    "with open(savepath + \"data/time_2.pkl\", \"wb\") as f:\n",
    "    pickle.dump((elapsed_time_engine, elapsed_time_processing), f, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
